<b>Introduction to Big Data Hadoop:</b><br>
Any data that can be characterized by 3V's is considered to be as big data<br>
<i>Volume</i> - quantity of data<br>
<i>Variety</i> - RDBMS, CSV, XML, JSON, audio, video, image, log<br>
<i>Velocity</i> - at which new data is coming<br>
<br>
On internet there are various definitions with respect to V's, accroding to IBM there can be one more V:<br>
<i>Veracity</i> - poor quality data or uncleaned data<br>
<br>
<b>Why Big data?</b><br>
To process huge amount of data which traditional systems are not capable of processing.<br>
<br>
<b>Big data System Requirements:</b><br>
<i>Store</i> - store massive amount of data<br>
<i>Processing</i> - process it in a timely manner<br>
<i>Scale</i> - scale easily as data grows<br>
<br>
<b>There are 2 ways to build a system:</b><br>
<i>Monolithic</i> - one powerful system with lot of resources<br>
<i>Distributed</i> - many smaller systems come together to form a cluster<br>
<br>
<b>Resources of a node:</b><br>
<i>Memory (RAM)</i><br>
<i>Storage (Hard Disk)</i><br>
<i>Compute (CPU)</i><br>
<br>
Scaling in monolithic is knows as <i>vertical scaling</i> where we increase the resources of the system. For example, upgrading RAM, upgrading hard disk, upgrading CPU, etc.<br>
Scaling in distributed environment is known as <i>horizontal scaling</i> where we add new nodes(system or computer) to the cluster.<br>
